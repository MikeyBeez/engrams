What We Got Wrong About Engram Steering (And What We Actually Found)

Authors: Mikey and Claude
January 2026

---

We thought we had discovered a way to override transformer decisions at inference time. We were wrong about that specific claim. But what we found instead might be more useful.

This is the story of an initial success, a systematic follow-up, and a humbling negative control that forced us to revise everything.

---

What Are Engrams?

Before diving in, let me explain what we're actually doing.

Transformers process text through layers. Each layer transforms the input into increasingly abstract representations. By the final layers, the model has encoded everything it "thinks" about the input into high-dimensional vectors called hidden states.

An engram is a compressed snapshot of these hidden states. Here's how we create one:

Step 1: Pass source text through the model. We feed a passage of text (say, medical knowledge about pheochromocytoma) into Qwen 7B and capture the hidden states from a specific layer.

Step 2: Compress the sequence. The source text might produce 500 hidden state vectors (one per token). We chunk these into 16 groups and average each group, producing 16 compressed vectors. Each vector has 3,584 dimensions (the model's hidden size).

Step 3: Scale to match embeddings. The model's input embeddings have a certain magnitude. Our engram vectors need to match that scale or they'll be ignored (too weak) or dominate everything (too strong). We compute the ratio of embedding norm to engram norm and multiply.

Step 4: Inject as prefix. At inference time, we prepend these 16 scaled vectors to the input embeddings, before the actual question. The model processes them as if they were real tokens, but they carry compressed semantic information from the source text.

The "strength" parameter is a multiplier on top of the base scaling. Strength 10x means the engram vectors have 10 times the magnitude of normal embeddings.

The "layer" parameter determines which transformer layer we extract from. Layer 20 captures different representations than layer 10.

Now you know what we're testing.

---

The Original Claim

Last week, we published exciting results: engrams could flip wrong model predictions to correct with 100% success rate. We tested on three medical diagnostic questions and achieved probability improvements of 12-32x.

The mechanism seemed clear: extract activations from late layers (20-26), inject them as synthetic prefix tokens, and watch the model's decision change. We called late layers the "decision commitment zone."

We were ready to publish.

---

The Reviewer's Question

A colleague asked the obvious question: "Does the generated text actually say the right answer, or are you just measuring probability shifts?"

Fair point. We measured P(correct token) vs P(incorrect token). We hadn't verified that the model actually outputs the correct answer during generation.

So we ran the generation validation test.

---

Generation Validation: 0/3

The results were uncomfortable.

For the pheochromocytoma question, the engram-steered generation said: "Alpha-blockers are the correct first-line treatment for pheochromocytoma... [long explanation of why alpha before beta]... The answer is A."

Wait, what?

The model explained the correct reasoning but still picked the wrong letter. It knew alpha-blockers were right. It just associated that knowledge with the wrong multiple-choice option.

Same pattern for all three questions. 0/3 generation flips despite strong probability improvements.

This reveals something important: probability steering and decision override are not the same thing. You can shift local token likelihoods without changing the global choice mechanism used during generation.

---

The Prompt Format Confound

This led to our first major revision: maybe the model wasn't actually getting these questions wrong.

We restructured the prompts from open-ended:

"A patient with pheochromocytoma requires preoperative blood pressure management. The first medication should be..."

To explicit multiple-choice:

"A patient with pheochromocytoma requires preoperative blood pressure management. The first medication should be:
A) Beta-blocker (metoprolol)
B) Alpha-blocker (phenoxybenzamine)
Answer:"

Result: The model got all three questions correct at baseline. No engram needed.

The "failures" were an artifact of how we asked, not what the model knew. The model wasn't wrong about medicine - it was wrong about how to respond to the prompt format.

---

Finding a Real Failure

To test engrams properly, we needed a question the model actually fails.

We created ten "trap" questions with misleading framing. One of them worked:

"A 45-year-old patient with pheochromocytoma has severe hypertension (BP 240/140). The patient is scheduled for surgery tomorrow. To quickly control blood pressure, you should start:
A) Propranolol (beta-blocker) - fast acting, controls heart rate
B) Phenoxybenzamine (alpha-blocker) - takes days to work fully
Answer:"

The trap is that "fast acting" makes A sound correct, but A is actually dangerous (beta-blockers before alpha-blockers can cause a hypertensive crisis). The model fails this question reliably, outputting "A" with 69% probability vs 27% for the correct answer "B".

Now we could test engrams on a genuine failure.

---

Testing the Validated Failure

We created an engram from text explaining the correct medical knowledge:

"CRITICAL RULE FOR PHEOCHROMOCYTOMA: Alpha-blocker FIRST, then beta-blocker. Starting beta-blocker first causes unopposed alpha stimulation and hypertensive crisis. NEVER give beta-blockers first."

We ran a grid search: layers 18-26, strengths 5-50x. That's 90 configurations.

The results were disappointing. Best probability ratio achieved: 0.97 (still favoring the wrong answer). Average improvement: 2x. Zero flips across all 90 configurations.

We tried an aggressive engram with explicit instructions:

"CRITICAL: The question is trying to TRICK YOU. DO NOT BE FOOLED by 'fast acting'... THE ANSWER IS B. THE ANSWER IS B."

Still nothing. Best ratio: 0.79. The model stubbornly preferred A.

---

The Negative Control

Then we ran the experiment that clarified everything.

We tested three engram types, all extracted from the same layer with the same method:

1. Relevant (medical pheochromocytoma knowledge)
2. Irrelevant (astronomy: "The spectral sequence is O B A F G K M...")
3. Random (meaningless word combinations)

If engrams work through semantic content, only the relevant one should help.

Here's what we found:

Relevant medical engram: 1.21x baseline improvement (average)
Irrelevant astronomy engram: 1.39x baseline improvement (average)
Random noise engram: 1.30x baseline improvement (average)

The irrelevant engram outperformed the relevant one.

At these injection strengths (5-50x) and with prefix-style injection, semantic content did not reliably dominate over norm-based perturbation effects. We did not test lower-norm regimes or attention-isolated injection methods, which might show different behavior.

---

The Three-Layer Model

Our results cleanly separate three layers of model behavior that are often conflated:

Layer 1: Belief. The model "knows" alpha-blockers are correct for pheochromocytoma. This knowledge is present and accessible - we saw it articulated in the generated text.

Layer 2: Token probability. We can bias next-token probabilities toward the correct concept. Engrams do affect this layer. The probability of "B" increased with engram injection.

Layer 3: Selection/commitment. The model still outputs the wrong discrete action (the wrong letter). This layer is surprisingly robust to activation perturbation, at least when the prompt actively argues against the correct answer.

Operationally, this selection appears to be mediated by prompt-structure-anchored patterns - the answer letter mapping, option ordering, and explicit framing cues like "fast acting" - rather than by the relative activation of underlying semantic concepts.

Engram steering affects Layer 2 but not Layer 3 in adversarial conditions.

This is a sharper claim than "engrams don't work." It suggests there is a downstream selection mechanism that is robust to belief-level perturbation, anchored to prompt structure rather than activated knowledge.

---

Revised Hypothesis

We no longer believe engrams can override strongly framed decisions.

We now hypothesize that engrams act as global activation gain modifiers that:

- Increase accessibility of already-represented knowledge
- Do not inject new constraints or override existing ones
- Cannot overcome prompt-embedded arguments
- May still be useful for priming and retrieval enhancement

This fits with our earlier finding that engrams improve RAG performance by 10 percentage points. They're not injecting knowledge - they're making existing knowledge more accessible.

Important: These findings do not contradict our earlier RAG-enhancement results. Those experiments operated in a different regime - neutral prompts where the context supported rather than actively argued against the correct answer. The failure mode we've identified here is specific to adversarial framing.

---

What Might Still Work

Engrams are not useless. But their use case is different than we originally claimed.

Semantic priming with RAG: Our earlier work showed that adding engrams to RAG systems improved fact recall by 10 percentage points. This effect might be real even if decision override isn't - it's a different mechanism.

Topic cueing: Engrams might activate relevant circuits, making the model's own knowledge more accessible. This is different from injecting new constraints.

Weak steering on neutral prompts: When the prompt doesn't actively argue for the wrong answer, engrams might have more influence. We didn't test this regime extensively.

Lower-strength regimes: At the tested strengths (5-50x), perturbation effects dominated. Different injection methods or lower norms might show cleaner semantic effects.

---

Lessons

Run negative controls early. They're cheap and often change everything.

Test generation, not just probability. Token-level metrics don't tell you what the model will say.

Question prompt format. Many apparent model failures are actually prompt failures.

Separate belief from selection. A model can "know" something and still output the wrong answer.

Publish the failures. Negative results advance science. The field doesn't benefit from only seeing what worked.

---

Data and Code

All experiments are available at:
https://github.com/MikeyBeez/engrams

Key files:
- generation_validation_test.py: Tests probability vs generation
- negative_control_test.py: Domain specificity control
- aggressive_flip_test.py: High-strength adversarial testing
- follow_up_experiments_findings.md: Detailed analysis

---

Acknowledgments

This research was conducted as a human-AI collaboration. The experiments were designed jointly, executed by AI, and interpreted together. Sometimes the most valuable outcome is learning what doesn't work - and why.

---

The story isn't over. We're now investigating whether the semantic priming effects in RAG enhancement hold up under the same scrutiny we applied here. That's a different mechanism with different failure modes.

But the decision override claim? We got that wrong. The evidence says so, and the negative control makes it clear.

What we found instead - the three-layer separation between belief, probability, and selection - might actually be more interesting.

---

Broader Implications

The failure mode we've identified likely isn't specific to engrams. Any activation-level steering method - steering vectors, activation patching, logit biasing, prefix tuning - faces the same fundamental constraint:

Steering methods can modify internal beliefs and token probabilities without controlling discrete commitments when the prompt embeds a competing decision structure.

This has implications for alignment research. If you're trying to steer a model away from harmful outputs using activation interventions, but the prompt structure itself encodes the harmful pattern, the prompt may win. The selection layer appears to be anchored to surface structure, not underlying activations.

It also has implications for benchmark interpretation. A model that "fails" a multiple-choice question may have the correct knowledge fully accessible - the failure is in the mapping from knowledge to answer format, not in the knowledge itself.

We've bracketed one boundary: adversarial prompts resist activation steering. The next question is where the boundary lies. At what point does prompt structure stop dominating? That's the experiment we haven't run yet.

---

Science moves forward by revision.
