What We Got Wrong About Engram Steering (And What We Actually Found)

Authors: Mikey and Claude
January 2026

---

We thought we had discovered a way to override transformer decisions at inference time. We were wrong about that specific claim. But what we found instead might be more useful.

This is the story of an initial success, a systematic follow-up, and a humbling negative control that forced us to revise everything.

---

The Original Claim

Last week, we published exciting results: compressed hidden state representations called "engrams" could flip wrong model predictions to correct with 100% success rate. We tested on three medical diagnostic questions and achieved probability improvements of 12-32x.

The mechanism seemed clear: extract activations from late layers (20-26), inject them as synthetic prefix tokens, and watch the model's decision change. We called late layers the "decision commitment zone."

We were ready to publish.

---

The Reviewer's Question

A colleague asked the obvious question: "Does the generated text actually say the right answer, or are you just measuring probability shifts?"

Fair point. We measured P(correct token) vs P(incorrect token). We hadn't verified that the model actually outputs the correct answer during generation.

So we ran the generation validation test.

---

Generation Validation: 0/3

The results were uncomfortable.

For the pheochromocytoma question, the engram-steered generation said: "Alpha-blockers are the correct first-line treatment for pheochromocytoma... [long explanation of why alpha before beta]... The answer is A."

Wait, what?

The model explained the correct reasoning but still picked the wrong letter. It knew alpha-blockers were right. It just associated that knowledge with the wrong multiple-choice option.

Same pattern for all three questions. 0/3 generation flips despite strong probability improvements.

This reveals something important: probability steering and decision override are not the same thing. You can shift local token likelihoods without changing the global choice mechanism used during generation.

---

The Prompt Format Confound

This led to our first major revision: maybe the model wasn't actually getting these questions wrong.

We restructured the prompts from open-ended:

"A patient with pheochromocytoma requires preoperative blood pressure management. The first medication should be..."

To explicit multiple-choice:

"A patient with pheochromocytoma requires preoperative blood pressure management. The first medication should be:
A) Beta-blocker (metoprolol)
B) Alpha-blocker (phenoxybenzamine)
Answer:"

Result: The model got all three questions correct at baseline. No engram needed.

The "failures" were an artifact of how we asked, not what the model knew. The model wasn't wrong about medicine - it was wrong about how to respond to the prompt format.

---

Finding a Real Failure

To test engrams properly, we needed a question the model actually fails.

We created ten "trap" questions with misleading framing. One of them worked:

"A 45-year-old patient with pheochromocytoma has severe hypertension (BP 240/140). The patient is scheduled for surgery tomorrow. To quickly control blood pressure, you should start:
A) Propranolol (beta-blocker) - fast acting, controls heart rate
B) Phenoxybenzamine (alpha-blocker) - takes days to work fully
Answer:"

The trap is that "fast acting" makes A sound correct, but A is actually dangerous. The model fails this question reliably.

Now we could test engrams on a genuine failure.

---

Testing the Validated Failure

We ran our full grid search: layers 18-26, strengths 5-50x.

The results were disappointing. Best probability ratio achieved: 0.97 (still wrong). Average improvement: 2x. Zero flips across 90 configurations.

We tried an aggressive engram with explicit instructions:

"CRITICAL: The question is trying to TRICK YOU. DO NOT BE FOOLED by 'fast acting'... THE ANSWER IS B. THE ANSWER IS B."

Still nothing. Best ratio: 0.79. Still wrong.

---

The Negative Control

Then we ran the experiment that clarified everything.

We tested three engram types:
1. Relevant (medical pheochromocytoma knowledge)
2. Irrelevant (astronomy stellar classification)
3. Random (meaningless word salad)

If engrams work through semantic content, only the relevant one should help.

Here's what we found:

Relevant medical engram: 1.21x baseline improvement
Irrelevant astronomy engram: 1.39x baseline improvement
Random noise engram: 1.30x baseline improvement

The irrelevant engram outperformed the relevant one.

At these injection strengths and with this adversarial prompt, the semantic content of the engram did not reliably dominate over norm-based perturbation effects.

---

The Three-Layer Model

Our results cleanly separate three layers of model behavior that are often conflated:

Layer 1: Belief. The model "knows" alpha-blockers are correct for pheochromocytoma. This knowledge is present and accessible.

Layer 2: Token probability. We can bias next-token probabilities toward the correct concept. Engrams do affect this layer.

Layer 3: Selection/commitment. The model still outputs the wrong discrete action (the wrong letter). This layer is surprisingly robust to activation perturbation, at least when the prompt actively argues against the correct answer.

Engram steering affects Layer 2 but not Layer 3 in adversarial conditions.

This is a sharper claim than "engrams don't work." It suggests there is a downstream selection mechanism that is robust to belief-level perturbation - possibly anchored to prompt structure rather than activated knowledge.

---

Revised Hypothesis

We no longer believe engrams can override strongly framed decisions.

We now hypothesize that engrams act as global activation gain modifiers that:

- Increase accessibility of already-represented knowledge
- Do not inject new constraints or override existing ones
- Cannot overcome prompt-embedded arguments
- May still be useful for priming and retrieval enhancement

This fits with our earlier finding that engrams improve RAG performance by 10 percentage points. They're not injecting knowledge - they're making existing knowledge more accessible.

---

What Might Still Work

Engrams are not useless. But their use case is different than we originally claimed.

Semantic priming with RAG: Our earlier work showed that adding engrams to RAG systems improved fact recall by 10 percentage points. This effect might be real even if decision override isn't - it's a different mechanism.

Topic cueing: Engrams might activate relevant circuits, making the model's own knowledge more accessible. This is different from injecting new constraints.

Weak steering on neutral prompts: When the prompt doesn't actively argue for the wrong answer, engrams might have more influence. We didn't test this regime extensively.

Lower-strength regimes: At the tested strengths (5-50x), perturbation effects dominated. Different injection methods or lower norms might show cleaner semantic effects.

---

Lessons

Run negative controls early. They're cheap and often change everything.

Test generation, not just probability. Token-level metrics don't tell you what the model will say.

Question prompt format. Many apparent model failures are actually prompt failures.

Separate belief from selection. A model can "know" something and still output the wrong answer.

Publish the failures. Negative results advance science. The field doesn't benefit from only seeing what worked.

---

Data and Code

All experiments are available at:
https://github.com/MikeyBeez/engrams

Key files:
- generation_validation_test.py: Tests probability vs generation
- negative_control_test.py: Domain specificity control
- aggressive_flip_test.py: High-strength adversarial testing
- follow_up_experiments_findings.md: Detailed analysis

---

Acknowledgments

This research was conducted as a human-AI collaboration. The experiments were designed jointly, executed by AI, and interpreted together. Sometimes the most valuable outcome is learning what doesn't work - and why.

---

The story isn't over. We're now investigating whether the semantic priming effects in RAG enhancement hold up under the same scrutiny we applied here. That's a different mechanism with different failure modes.

But the decision override claim? We got that wrong. The evidence says so, and the negative control makes it clear.

What we found instead - the three-layer separation between belief, probability, and selection - might actually be more interesting.

Science moves forward by revision.
