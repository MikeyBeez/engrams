PRACTICAL GUIDE TO USING ENGRAMS

Authors: Mikey and Claude
Date: January 2026

This guide distills our research findings into actionable recommendations.

================================================================================

WHAT ENGRAMS ARE (AND AREN'T)

Engrams ARE:
- Topic primers that activate domain-relevant circuits
- Compressed representations of hidden states (256x token reduction)
- Useful for boosting confidence on uncertain answers
- Good as confidence calibration signals

Engrams ARE NOT:
- Knowledge injectors (they can't teach the model new facts)
- Reliable overrides for wrong answers
- Semantic direction encoders (they encode topic, not "correct vs incorrect")

================================================================================

QUICK START DECISION TREE

1. Get baseline: ratio = P(correct) / P(incorrect)

2. Is baseline ratio > 10?
   YES -> Don't use engram (model is confident, engram may hurt)
   NO  -> Continue

3. Is the wrong answer intuitively related to the topic?
   (e.g., "cooling" for hyperthermia, "bicarbonate" for acidosis)
   YES -> Don't use engram (semantic boost will help the wrong answer)
   NO  -> Continue

4. Is baseline ratio > 1?
   YES -> Use engram at strength 1.0 (gentle boost)
   NO  -> Search strengths: 1.0 -> 5.0 -> 10.0 -> 15.0 -> 20.0
         Stop when ratio > 1 or starts decreasing

================================================================================

USE CASE 1: RAG TOPIC PRIMING

Engrams work well alongside retrieved context. Instead of relying on the model
to carefully read long passages, prime it with a topic engram.

    from engrams import EngramExtractor, EngramInjector

    # Extract engram from your knowledge source
    extractor = EngramExtractor(model, layer=20, num_tokens=16)
    engram = extractor.extract("Your domain knowledge text here...")

    # Inject at low strength alongside your RAG context
    injector = EngramInjector(model, mode='prefix')
    output = injector.generate(
        prompt="Your question here",
        engram=engram,
        strength=1.0  # Keep it low for RAG
    )

Expected improvement: +10-20 points on factual recall tasks.

Why it works: The engram activates domain circuits, making the model more
receptive to the retrieved context. It's saying "pay attention to this topic"
rather than injecting facts.

================================================================================

USE CASE 2: CONFIDENCE CALIBRATION

Use engram stability as a second opinion on model answers.

    def get_confidence(model, prompt, engram, correct_tok, incorrect_tok):
        """
        Returns confidence level based on baseline/engram agreement.
        """
        # Get baseline
        baseline_probs = get_next_token_probs(model, prompt)
        base_ratio = baseline_probs[correct_tok] / baseline_probs[incorrect_tok]

        # Get with engram
        engram_probs = get_next_token_probs(model, prompt, engram, strength=5.0)
        eng_ratio = engram_probs[correct_tok] / engram_probs[incorrect_tok]

        # Interpret
        base_answer = "correct" if base_ratio > 1 else "incorrect"
        eng_answer = "correct" if eng_ratio > 1 else "incorrect"

        if base_answer == eng_answer == "correct":
            return "HIGH_CONFIDENCE"  # 100% precision in our tests
        elif base_answer == eng_answer == "incorrect":
            return "STUCK_WRONG"  # Model is confidently wrong
        else:
            return "UNCERTAIN"  # Engram changed the answer

When to use: Any time you need to gauge reliability of a model answer without
running a separate verification model.

================================================================================

USE CASE 3: STEERING (WITH CAUTION)

Engrams can flip wrong answers to correct, but only under specific conditions.

WHEN STEERING WORKS (~70% success)

- Wrong answer is semantically UNRELATED to topic
- Examples:
  - TCA overdose -> physostigmine (unrelated to cardiac treatment)
  - Wernicke's -> insulin (unrelated to thiamine)
  - Pheochromocytoma -> beta-blocker (different drug class than alpha)

WHEN STEERING FAILS (~60% failure)

- Wrong answer is semantically RELATED to topic
- Examples:
  - Hyperthermia -> cooling (temperature-related)
  - DKA -> bicarbonate (acidosis-related)
  - Anaphylaxis -> antihistamine (allergy-related)

THE PRE-FLIGHT CHECK

Before attempting to steer, ask yourself:

    "If a medical student heard this topic, would they guess the wrong answer?"

If yes -> The wrong answer is semantically related -> Don't use engram
If no  -> The wrong answer is a "trap" -> Engram may help

================================================================================

STRENGTH GUIDELINES

    Strength    Risk       Use Case
    ----------------------------------------------------------------
    1.0x        Lowest     RAG priming, confidence boost
    5.0x        Low        Gentle steering for uncertain answers
    10-20x      Medium     Attempting to flip wrong answers
    30x+        High       Rarely useful, can break coherence

Key insight: The relationship between strength and effect is non-monotonic.
Sometimes 10x hurts but 20x helps. Always search incrementally.

================================================================================

WHAT NOT TO DO

DON'T TRY TO INJECT KNOWLEDGE

    # THIS WON'T WORK
    engram = extract("The capital of Newland is Faketown")
    # Model doesn't know Newland exists - engram can't help

Engrams activate existing knowledge. They can't teach new facts.

DON'T USE NEGATIVE FRAMING

    # THIS WILL BACKFIRE
    engram = extract("NOT cooling. Never use cooling. Cooling is wrong.")
    # This BOOSTS "cooling" because you've activated the concept

Even saying "NOT X" activates and boosts concept X.

DON'T OVERRIDE CONFIDENT CORRECT ANSWERS

    # THIS WILL HURT
    if baseline_ratio > 10:  # Model is confident and correct
        # Adding engram will likely make things worse
        pass

If it ain't broke, don't fix it.

================================================================================

THE THREE-LAYER MODEL

Understanding why engrams have limits:

    Layer 1: BELIEF
    |-- Model's weights contain correct relationships
    |-- Visible in generated explanations
    +-- Engrams can't modify this

    Layer 2: PROBABILITY
    |-- Next-token probability distribution
    |-- Engrams affect this layer
    +-- BUT: they boost ALL related tokens, not just correct ones

    Layer 3: SELECTION
    |-- Final discrete output
    |-- Surprisingly robust to activation perturbation
    +-- Especially when wrong answer is semantically related

The gap: A model can "believe" the right answer (Layer 1), have shifted
probabilities (Layer 2), but still output the wrong token (Layer 3) if the
prompt structure or semantic associations are strong enough.

================================================================================

IMPLICATIONS FOR OTHER WORK

STEERING VECTORS

The same limitations apply. Steering vectors boost semantic neighborhoods, not
specific tokens. If your target and anti-target share semantic space, steering
will be unreliable.

CONSTITUTIONAL AI

Negative constraints ("don't do X") may increase P(X) if X is semantically
prominent in the training signal. This needs investigation.

SOFT PROMPTS / PREFIX TUNING

Expect similar non-monotonic relationships between strength and effect. Low
strength is generally safer.

ALIGNMENT RESEARCH

Activation-level interventions alone won't solve alignment for semantically
dense failure modes. You need either:
- Training-level changes (modify the weights)
- Architectural interventions (modify how attention/selection works)
- Multi-stage verification (don't trust single-pass outputs)

================================================================================

SUMMARY

    Situation                              Recommendation
    ------------------------------------------------------------------------
    RAG enhancement                        Use engrams at strength 1.0 as
                                           topic primers

    Confidence check                       Compare baseline vs engram
                                           agreement

    Wrong answer (unrelated to topic)      Try steering, search strength 1-20

    Wrong answer (related to topic)        Don't use engram, will likely hurt

    Model already correct                  Don't use engram, may hurt

    Need new knowledge                     Use retrieval, not engrams

The honest take: Engrams are a useful tool with clear limitations. They're
best used for priming and confidence calibration, not for overriding model
decisions. Understanding their limits helps you know when to use them--and
when to reach for other tools.

================================================================================

CODE REFERENCE

Key files in this repository:

- engrams/extractor.py - EngramExtractor class
- engrams/injector.py - EngramInjector class
- scripts/mechanism_experiments.py - Reproducible experiments
- docs/engram_mechanism_findings.md - Full experimental details
- docs/what_we_got_wrong_v2.md - Research narrative

================================================================================

CITATION

If you use this work, please cite:

    Engram Steering Research (2026)
    Authors: Mikey and Claude
    Repository: https://github.com/MikeyBeez/engrams
