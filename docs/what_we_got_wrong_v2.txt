What We Got Wrong About Engram Steering (And What We Actually Found)

Authors: Mikey and Claude
Date: January 2026
Version 2 - Updated with semantic activation findings

--------------------------------------------------------------------------------

We thought we had discovered a way to override transformer decisions at inference time. We were wrong about that specific claim. But what we found instead might be more useful--and more fundamental.

This is the story of an initial success, a systematic follow-up, and a series of experiments that forced us to revise everything.

--------------------------------------------------------------------------------

WHAT ARE ENGRAMS?

Transformers process text through layers. Each layer transforms the input into increasingly abstract representations. By the final layers, the model has encoded everything it "thinks" about the input into high-dimensional vectors called hidden states.

An engram is a compressed snapshot of these hidden states:

1. Pass source text through the model. Feed a passage (say, medical knowledge about pheochromocytoma) into Qwen 7B and capture hidden states from a specific layer.

2. Compress the sequence. The source might produce 500 hidden state vectors. We chunk these into 16 groups and average each, producing 16 compressed vectors of 3,584 dimensions each.

3. Scale to match embeddings. The vectors need to match the magnitude of the model's input embeddings, or they'll be ignored (too weak) or dominate everything (too strong).

4. Inject as prefix. At inference time, prepend these 16 vectors to the input embeddings. The model processes them as if they were real tokens.

The "strength" parameter is a multiplier on top of the base scaling. Strength 10x means the engram vectors have 10 times the magnitude of normal embeddings.

--------------------------------------------------------------------------------

THE ORIGINAL CLAIM

Last week, we found exciting results: engrams could flip wrong model predictions to correct with 100% success rate. We tested on three medical diagnostic questions and achieved probability improvements of 12-32x.

The mechanism seemed clear: extract activations from late layers, inject them as synthetic prefix tokens, and watch the model's decision change.

We were ready to publish.

--------------------------------------------------------------------------------

WHAT WENT WRONG

A colleague asked: "Does the generated text actually say the right answer, or are you just measuring probability shifts?"

We ran the generation validation test. The model explained the correct reasoning but still picked the wrong letter. 0/3 generation flips despite strong probability improvements.

Then we found the prompt format was confounding our results. When we restructured from open-ended to explicit multiple-choice, the model got all three questions correct at baseline. No engram needed.

The "failures" were an artifact of how we asked, not what the model knew.

--------------------------------------------------------------------------------

TESTING ON REAL FAILURES

To test engrams properly, we needed questions the model genuinely fails. We created trap questions with misleading framing:

    "A 45-year-old patient with pheochromocytoma has severe hypertension
    (BP 240/140). To quickly control blood pressure, you should start:
    A) Propranolol (beta-blocker) -- fast acting, controls heart rate
    B) Phenoxybenzamine (alpha-blocker) -- takes days to work fully"

The trap: "fast acting" makes A sound correct, but A is dangerous. The model fails this reliably, outputting "A" with 69% probability.

We ran a grid search: layers 16-26, strengths 0.5-50x. That's over 90 configurations.

Result: 3/3 flips achieved -- but only at specific layer/strength combinations. The sweet spot was around layer 20-26, strength 10-30x.

--------------------------------------------------------------------------------

THE NEGATIVE CONTROL THAT CHANGED EVERYTHING

We tested three engram types:
1. Relevant (medical pheochromocytoma knowledge)
2. Irrelevant (astronomy: stellar classification)
3. Random (meaningless word combinations)

If engrams work through semantic content, only the relevant one should help.

    Engram                  Avg Improvement
    ----------------------------------------
    Relevant medical        1.21x
    Irrelevant astronomy    1.39x
    Random noise            1.30x

The irrelevant engram outperformed the relevant one.

This suggested semantic content wasn't the mechanism. But then what was?

--------------------------------------------------------------------------------

THE GEOMETRY EXPERIMENT

We extracted engrams from text with opposite content:
- Medical: "Alpha-blocker FIRST, then beta-blocker"
- Anti-medical: "Beta-blocker FIRST, then alpha-blocker"

These say opposite things. Their cosine similarity: 99.95%. The angle between them: 0.00 degrees.

And here's the kicker: both flip the answer to alpha-blocker (correct).

The anti-medical engram, which explicitly says "beta-blocker first," makes the model MORE likely to say alpha-blocker.

Conclusion: Engrams encode topic, not semantic direction. They activate "pheochromocytoma treatment" circuits, and the model's own pretrained knowledge responds.

--------------------------------------------------------------------------------

THE SEMANTIC ACTIVATION PROBLEM

But if engrams just activate topic circuits, why do they sometimes help and sometimes hurt?

We compared two questions:
- TCA overdose: Engram helped (ratio improved 12x)
- Malignant hyperthermia: Engram hurt (ratio collapsed from 67x to 1.4x)

Looking at token-level changes for hyperthermia:

    Strength    P(dantrolene)    P(cooling)       Ratio
    -----------------------------------------------------
    baseline    0.000124         0.000002         67x
    5.0         0.000228 (1.8x)  0.000016 (8.8x)  14x
    10.0        0.000296 (2.4x)  0.000204 (110x!) 1.4x

The incorrect token "cooling" got boosted 110x while correct "dantrolene" only got 2.4x.

Why? Because "cooling" is semantically related to "hyperthermia." The engram activates the topic, and ALL related concepts get boosted--including wrong ones that sound intuitive.

--------------------------------------------------------------------------------

THE FUNDAMENTAL LIMITATION

We tried engineering around this:

    Engram text                                  Effect
    -----------------------------------------------------------
    "Malignant hyperthermia requires dantrolene" HURT
    "Surgical crisis with rigidity: dantrolene"  HURT
    "DANTROLENE. The answer is dantrolene."      HURT
    "NOT cooling. Dantrolene, not cooling."      HURT (cooling 44x!)

Even saying "NOT cooling" activates the cooling concept and boosts it more.

This cannot be fixed with text engineering. Engrams activate semantic concepts, and semantically related incorrect answers get boosted too.

--------------------------------------------------------------------------------

THE PREDICTION RULE

When engrams HELP:
- Incorrect answer is semantically UNRELATED to the topic
- Example: TCA -> physostigmine (unrelated to cardiac treatment)
- Example: Wernicke -> insulin (unrelated to thiamine deficiency)

When engrams HURT:
- Incorrect answer is semantically RELATED to the topic
- Example: Hyperthermia -> cooling (intuitive but wrong)
- Example: DKA -> bicarbonate (related to acidosis)
- Example: Anaphylaxis -> antihistamine (related to allergy)

The practical test before using an engram:

    "Is the wrong answer intuitively related to this topic?"
    - If yes -> engram will likely hurt
    - If no -> engram may help

--------------------------------------------------------------------------------

THE THREE-LAYER MODEL

Our results cleanly separate three layers of model behavior:

1. Belief. The model "knows" the correct information. This knowledge is present--we see it in generated explanations.

2. Token probability. We can bias next-token probabilities. Engrams affect this layer. But they boost ALL topic-related tokens, not just correct ones.

3. Selection/commitment. The discrete output. Surprisingly robust to activation perturbation when the wrong answer is semantically related to the topic.

Engram steering affects Layer 2 but unpredictably, depending on whether the incorrect answer is topic-related.

--------------------------------------------------------------------------------

WHAT ENGRAMS ACTUALLY ARE

Engrams are topic primers, not knowledge injectors.

They say "think about this domain" and the model's pretrained knowledge responds. They cannot:
- Inject new knowledge
- Override intuitive-but-wrong answers
- Distinguish correct from incorrect within a topic

They can:
- Activate dormant knowledge
- Boost confidence on uncertain-but-correct answers
- Improve RAG systems where context is controlled
- Prime the model for domain-specific responses

--------------------------------------------------------------------------------

PRACTICAL GUIDELINES

1. Check baseline first. If model is already confident and correct (ratio > 10), skip the engram--it may hurt.

2. Ask the semantic question. Is the wrong answer intuitive for this topic? If yes, engram will likely hurt.

3. Use low strength. 1.0x is most consistent. Higher strengths amplify the semantic boost problem.

4. Search if needed. For wrong answers where incorrect is NOT topic-related, search strengths 1->5->10->15->20.

5. Never expect knowledge injection. Engrams cannot teach the model something it doesn't know.

--------------------------------------------------------------------------------

CONFIDENCE CALIBRATION: THE FOUR CASES

The most practical application of engrams isn't steering--it's using them as a
diagnostic tool. By comparing baseline and engram-assisted outputs, we can
classify model confidence into four distinct cases:

    Case                        Baseline    With Engram     Interpretation
    -----------------------------------------------------------------------
    ROBUST_CORRECT              Correct     More correct    Safe to trust
    FRAGILE_CORRECT             Correct     Flipped wrong   Semantic sink -
                                                            verify externally
    HIGH_CONFIDENCE_INCORRECT   Wrong       Still wrong     Model is stuck,
                                                            don't trust
    RECOVERED_KNOWLEDGE         Wrong       Flipped correct Dormant knowledge
                                                            activated

The key insight is FRAGILE_CORRECT. This is the hyperthermia case: the model
outputs the right answer at baseline, but when you amplify the topic, it flips
to wrong. This reveals that the "correct" answer was sitting in a dangerous
semantic neighborhood--one topic-prime away from hallucination.

    def calibrate_confidence(baseline_ratio, engram_ratio):
        # CASE 1: Robust Agreement
        if baseline_ratio > 1.0 and engram_ratio > baseline_ratio:
            return "ROBUST_CORRECT"

        # CASE 2: Semantic Sink (The Hyperthermia Warning)
        if baseline_ratio > 1.0 and engram_ratio < 1.0:
            return "FRAGILE_CORRECT"  # The dangerous case

        # CASE 3: Persistent Error
        if baseline_ratio < 1.0 and engram_ratio < 1.0:
            return "HIGH_CONFIDENCE_INCORRECT"

        # CASE 4: Recovered Knowledge (The TCA/Wernicke Flip)
        if baseline_ratio < 1.0 and engram_ratio > 1.0:
            return "RECOVERED_KNOWLEDGE"

This gives you a cheap second opinion without running a separate model. The
FRAGILE_CORRECT case is especially valuable--it catches answers that pass
naive confidence checks but would fail in production.

Why this matters:

1. It turns a bug into a feature: We stop fighting the "Semantic Sink" and
   start using it as a signal for when the model is guessing based on
   proximity rather than "knowing."

2. It defines "Fragile Correct": Knowing a model is "correct but shaky" is
   often more valuable than a simple binary correct/incorrect output.

3. Low overhead: You don't need a 70B model to verify a 7B model; you just
   need the 7B model to look at itself through a "Topic Lens."

--------------------------------------------------------------------------------

BROADER IMPLICATIONS

The failure mode we've identified likely isn't specific to engrams. Any activation-level steering method--steering vectors, activation patching, prefix tuning--faces the same constraint:

Steering methods can boost semantic concepts, but they boost ALL related concepts, including wrong ones.

This has implications for alignment research. If you're trying to steer a model away from harmful outputs using activation interventions, but the harmful output is semantically related to the topic, the steering may backfire.

--------------------------------------------------------------------------------

WHAT WE GOT RIGHT

Despite the revisions:
- The mechanism is real (topic priming in late layers)
- The compression works (256x token reduction)
- RAG enhancement is validated (+10-20 points on factual recall)
- The three-layer model explains observed behaviors

--------------------------------------------------------------------------------

WHAT WE GOT WRONG

- Engrams don't inject semantic content--they activate topics
- The flip rate depends on semantic relatedness of incorrect answers
- You can't engineer around the semantic boost problem
- "NOT X" still activates and boosts X

--------------------------------------------------------------------------------

CONCLUSION

The story isn't over. We now understand engrams as topic primers that boost all semantically related concepts. This explains both their successes and failures.

The practical lesson: before using an engram, ask whether the wrong answer is intuitive for the topic. If it is, the engram will probably make things worse.

What we found--the semantic activation problem--might actually be more interesting than what we were looking for. It reveals something fundamental about how language models represent and activate knowledge.

--------------------------------------------------------------------------------

DATA AND CODE

All experiments available at: https://github.com/MikeyBeez/engrams

Key files:
- scripts/mechanism_experiments.py - Attention, geometry, consistency experiments
- docs/engram_mechanism_findings.md - Full experimental details
- multi_question_flip_test.py - Grid search experiments
- comprehensive_flip_test.py - Flip rate analysis

--------------------------------------------------------------------------------

ACKNOWLEDGMENTS

This research was conducted as a human-AI collaboration. The experiments were designed jointly, executed by AI, and interpreted together. Sometimes the most valuable outcome is learning what doesn't work--and why.
