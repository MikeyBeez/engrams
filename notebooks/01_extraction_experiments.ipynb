{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engram Extraction Experiments\n",
    "\n",
    "This notebook explores extracting dense representations from transformer hidden states\n",
    "to compress knowledge into token-sized engrams.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "1. **Which layer captures the best semantic representation?**\n",
    "2. **What pooling strategy preserves the most information?**\n",
    "3. **How much compression can we achieve while retaining usefulness?**\n",
    "4. **Can injected engrams improve model responses about specific entities?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engrams import EngramExtractor, EngramInjector, EngramStore\n",
    "from engrams.extractor import ExtractionConfig\n",
    "from engrams.wikipedia import WikipediaEngramBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Extraction\n",
    "\n",
    "Let us start by extracting an engram from a Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure extraction\n",
    "# Using Llama 3.2 1B for quick experiments (fits in 16GB easily)\n",
    "config = ExtractionConfig(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "    layer=\"middle\",\n",
    "    pooling=\"mean\",\n",
    "    num_engram_tokens=4,\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "extractor = EngramExtractor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create builder (without store for now)\n",
    "builder = WikipediaEngramBuilder(extractor=extractor, store=None)\n",
    "\n",
    "# Fetch article\n",
    "text = builder.fetch_article(\"Abraham Lincoln\")\n",
    "print(f\"Article length: {len(text)} characters\")\n",
    "print(f\"\\nFirst 500 chars:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract engram\n",
    "engram = extractor.extract(text)\n",
    "print(f\"\\nEngram: {engram}\")\n",
    "print(f\"Vector shape: {engram.vectors.shape}\")\n",
    "print(f\"Original tokens: {engram.source_length}\")\n",
    "print(f\"Compression ratio: {engram.compression_ratio:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer Comparison\n",
    "\n",
    "Which layer produces the best semantic representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Test different layers\n",
    "layers_to_test = [0, 4, 8, 12, 15]\n",
    "engrams_by_layer = {}\n",
    "\n",
    "for layer in layers_to_test:\n",
    "    config = ExtractionConfig(\n",
    "        model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "        layer=layer,\n",
    "        num_engram_tokens=4,\n",
    "    )\n",
    "    ext = EngramExtractor(config)\n",
    "    engrams_by_layer[layer] = ext.extract(text)\n",
    "    print(f\"Layer {layer}: shape={engrams_by_layer[layer].vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize vector norms across layers\n",
    "fig, axes = plt.subplots(1, len(layers_to_test), figsize=(15, 3))\n",
    "\n",
    "for i, layer in enumerate(layers_to_test):\n",
    "    vectors = engrams_by_layer[layer].vectors.numpy()\n",
    "    norms = np.linalg.norm(vectors, axis=1)\n",
    "    axes[i].bar(range(len(norms)), norms)\n",
    "    axes[i].set_title(f\"Layer {layer}\")\n",
    "    axes[i].set_xlabel(\"Engram token\")\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel(\"L2 norm\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../data/layer_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compression Ratio Experiments\n",
    "\n",
    "How much can we compress while retaining useful information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different compression levels\n",
    "num_tokens_to_test = [1, 2, 4, 8, 16, 32]\n",
    "compression_results = {}\n",
    "\n",
    "for num_tokens in num_tokens_to_test:\n",
    "    config = ExtractionConfig(\n",
    "        model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "        layer=\"middle\",\n",
    "        num_engram_tokens=num_tokens,\n",
    "    )\n",
    "    ext = EngramExtractor(config)\n",
    "    eng = ext.extract(text)\n",
    "    compression_results[num_tokens] = eng\n",
    "    print(f\"{num_tokens} tokens â†’ {eng.compression_ratio:.1f}x compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Injection Test\n",
    "\n",
    "Does injecting the engram affect generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engrams.injector import InjectionConfig\n",
    "\n",
    "# Use prefix injection (simpler than replace)\n",
    "inject_config = InjectionConfig(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "    injection_mode=\"prefix\",\n",
    ")\n",
    "\n",
    "injector = EngramInjector(inject_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: generate WITHOUT engram\n",
    "prompt = \"Abraham Lincoln was\"\n",
    "\n",
    "tokens = injector.tokenizer(prompt, return_tensors=\"pt\").to(injector.model.device)\n",
    "with torch.no_grad():\n",
    "    baseline_output = injector.model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "baseline_text = injector.tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "print(\"BASELINE (no engram):\")\n",
    "print(baseline_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH engram injected\n",
    "engram_text = injector.inject_and_generate(\n",
    "    prompt=prompt,\n",
    "    engram=engram,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(\"\\nWITH ENGRAM:\")\n",
    "print(engram_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "- [ ] Quantitative evaluation on QA tasks\n",
    "- [ ] Compare pooling strategies (mean vs attention vs learned)\n",
    "- [ ] Test with larger models (Llama 3.2 3B, 8B)\n",
    "- [ ] Build entity database from Wikipedia\n",
    "- [ ] Measure hallucination reduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
