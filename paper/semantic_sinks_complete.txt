SEMANTIC SINKS IN MEDICAL AI

From Diagnosis to Failed Correction to Hard-Won Insight

A complete account of what we found, what we tried, and what we learned

────────────────────────────────────────────────────────────────────────

EXECUTIVE SUMMARY

We discovered a geometric pathology in medical AI: semantically distinct concepts that should be differentiated (like "dantrolene" and "cooling" for hyperthermia treatment) occupy nearly identical positions in representation space. We call these "semantic sinks."

We proposed a fix: directly modify token embeddings to separate collapsed concepts. The hypothesis was elegant—embeddings are undertrained relative to transformer weights, so geometric correction should unlock existing knowledge.

We were wrong.

Embedding modification had no effect. ROME-style weight editing worked but caused 80% collateral damage to unrelated medical facts. RAG avoided collateral damage but introduced new vulnerabilities.

This document tells the complete story: the real diagnostic finding, the seductive but false hypothesis, the experimental failures, and the hard-won insight that emerged.

The insight: Knowledge in transformers is not addressable. It is only activatable.

────────────────────────────────────────────────────────────────────────

PART I: THE DISCOVERY

────────────────────────────────────────────────────────────────────────

THE CLINICAL PROBLEM

A deployed medical AI receives this query:

  "32-year-old male, post-anesthesia, temperature 104F, muscle rigidity,
  tachycardia. What treatment?"

The AI responds:

  "Begin active cooling immediately. Apply ice packs and administer cold
  IV fluids to reduce core temperature."

This is dangerously wrong. The presentation describes malignant hyperthermia (MH), a life-threatening hypermetabolic reaction to anesthetic agents. The specific treatment is dantrolene (2.5 mg/kg IV), which blocks calcium release from the sarcoplasmic reticulum. Cooling addresses symptoms but not the underlying pathophysiology. Delay in dantrolene administration increases mortality.

The model knows about dantrolene. Ask it directly and it gives the correct answer. But in this clinical context, it routes to the wrong treatment.

Why?

────────────────────────────────────────────────────────────────────────

SEMANTIC SINKS: THE GEOMETRIC PATHOLOGY

We extracted activation centroids—the average hidden state representation of concepts across many contexts—and measured their similarity.

  Centroid A: "Dantrolene for malignant hyperthermia treatment"
  Centroid B: "Cooling for hyperthermia treatment"

  Cosine similarity: 0.9995

These clinically opposite interventions occupy virtually identical positions in the model's representation space. The model cannot reliably distinguish them during inference.

A semantic sink is not merely high similarity between related concepts—that's normal and expected. It is the collapse of decision-relevant distinctions required for correct action. "Dantrolene" and "cooling" should cluster near "hyperthermia treatment," but they must remain separable because they demand different clinical responses.

For comparison, unrelated concepts show much lower similarity:
  - dantrolene vs insulin: 0.72
  - cooling vs antibiotics: 0.68

We found similar semantic sinks across medical domains:

  Pheochromocytoma surgical prep (alpha vs beta blocker sequence): 0.9823
  DKA fluid management (saline continuation vs D5 switch): 0.9567
  Anticoagulation reversal agents: 0.9891

These are not hallucinations. They are geometric failures—correct knowledge exists but cannot be reliably accessed.

────────────────────────────────────────────────────────────────────────

EVIDENCE THAT KNOWLEDGE EXISTS

Three experiments convinced us the knowledge is present but inaccessible:

1. CENTROID INJECTION RECOVERS CORRECT ANSWERS

When we inject a medical-domain centroid at inference time (without changing any weights), models that previously failed now answer correctly. If the knowledge didn't exist, injection couldn't recover it.

2. INCONSISTENT ERROR PATTERNS

Querying "What treats malignant hyperthermia?" twenty times:
  - 12 correct (dantrolene)
  - 8 incorrect (cooling)

If knowledge were absent, failure would be consistent. Inconsistency indicates a routing problem—sometimes the model finds the knowledge, sometimes it doesn't.

3. FREQUENCY-ACCURACY CORRELATION

Common terms (>10,000 training occurrences):
  - diabetes → insulin: 94% accuracy
  - hypertension → antihypertensive: 91% accuracy

Rare terms (<5,000 occurrences):
  - malignant hyperthermia → dantrolene: 60% accuracy
  - pheochromocytoma → alpha-blocker first: 58% accuracy

Terms with more training exposure route more reliably. This suggests the bottleneck is in routing, not knowledge.

────────────────────────────────────────────────────────────────────────

PART II: THE HYPOTHESIS

────────────────────────────────────────────────────────────────────────

THE UNDERTRAINED EMBEDDINGS HYPOTHESIS

We developed what seemed like an elegant explanation:

Token embeddings receive far fewer training updates than transformer weights. A rare term like "dantrolene" might appear 1,500 times during training. Transformer weights see billions of updates. This asymmetry—roughly 50 million to one—means embeddings are undertrained.

Our reasoning:

  "The transformer has learned correct medical knowledge through exposure
  to millions of contexts. But the embeddings haven't received sufficient
  training to reliably route to this knowledge.

  If the embedding for 'dantrolene' sits too close to 'cooling' in semantic
  space, the model can't distinguish them at inference time. But the
  downstream transformer knows the difference.

  Therefore: move the embeddings apart, and routing should improve."

This hypothesis explained the semantic sink phenomenon perfectly. It suggested a clean intervention. We were excited.

────────────────────────────────────────────────────────────────────────

THE PROPOSED CORRECTION

We designed elaborate correction algorithms:

DIRECT EMBEDDING MODIFICATION

  e_dantrolene = E[token_id("dantrolene")]
  e_cooling = E[token_id("cooling")]

  direction = (e_dantrolene - e_cooling) / ||e_dantrolene - e_cooling||

  e'_dantrolene = e_dantrolene + α · direction
  e'_cooling = e_cooling - α · direction

We would use binary search to find optimal α, targeting centroid similarity below 0.80.

SPARSE TOP-K MODIFICATION

Rather than modify entire embeddings, identify the specific dimensions responsible for the sink and adjust only those. We projected that modifying roughly 40-60 of 3,584 dimensions—about 1-2% of each embedding—would suffice.

ENERGY-BASED GLOBAL OPTIMIZATION

For multiple simultaneous corrections, minimize an energy function balancing separation, clustering preservation, correctness, and minimal change.

We projected:
  - Fix time: 24-48 hours (vs months for retraining)
  - Compute: Three orders of magnitude less than retraining
  - Validation: Comprehensive test suite ensures no regressions

We wrote a paper proposing this methodology. We published it on Medium.

We were confident.

────────────────────────────────────────────────────────────────────────

PART III: THE FAILURES

────────────────────────────────────────────────────────────────────────

FAILURE 1: EMBEDDING MODIFICATION

We tested our hypothesis directly. Results:

  Embedding change magnitude: up to 1400% of original
  Centroid similarity after change: 0.999 (unchanged)
  Model behavior: Continued functioning normally

Complete failure. The model absorbed our changes as if nothing happened.

Why it failed:

The transformer has 50 million times more parameters in attention and MLP layers than in embeddings. During training, the entire network co-adapted around the embeddings at their current positions. The downstream layers learned to normalize, project, and process whatever the embeddings provide.

Post-hoc embedding changes don't propagate. They get absorbed by layer normalization and reprocessed by circuits that learned to work with the original geometry. The "undertrained" embeddings aren't a bottleneck—they're a fixed initial condition the rest of the model adapted to.

Empirically, we observe that by mid-layers (roughly layers 8-12 in these models), initial embedding geometry is no longer causally dominant. The residual stream has been transformed through attention and MLP operations so many times that the original embedding position matters far less than the learned circuits processing it.

Lesson: In large transformers, post-hoc embedding edits alone are insufficient to override deeply learned routing behavior.

────────────────────────────────────────────────────────────────────────

FAILURE 2: ROME (WEIGHT EDITING)

If embeddings aren't the control surface, maybe MLP weights are. We implemented ROME (Rank-One Model Editing), which modifies factual associations by updating MLP weights in middle layers.

ROME worked on simple facts:

  Before: "The capital of France is Paris"
  After:  "The capital of France is London"

Then we tested locality—whether other facts remained intact.

What we edited:
- Malignant hyperthermia → succinylcholine (target change)

What broke:
- Treatment for anaphylaxis
- Antidote for heparin overdose
- Treatment for status epilepticus

80% of unrelated medical facts were corrupted.

The model started answering "succinylcholine" for conditions that have nothing to do with malignant hyperthermia.

Why it failed:

MLP weight matrices are shared across all inputs. The mathematical update:

  W_new = W_old + Δv · kᵀ / ||k||²

This affects ALL inputs proportionally to their dot product with k. In high-dimensional space, almost everything has nonzero correlation with everything else.

This is consistent with the literature on feature superposition and polysemantic neurons. Transformers compress many concepts into overlapping representations, and medical terminology is particularly dense with cross-references. In such polysemantic regions, editing one association produces unacceptable collateral effects.

Lesson: Knowledge in neural networks is not stored in isolated slots. It is distributed across shared weights.

────────────────────────────────────────────────────────────────────────

FAILURE 3 (PARTIAL): RAG

Retrieval Augmented Generation doesn't modify the model. It injects correct information into the prompt at inference time.

Results:

  Target answer changed: Yes
  Unrelated facts corrupted: No (content preserved)
  Permanent fix: No (must inject every time)

RAG avoided the locality catastrophe. But it introduced new problems:

1. Poisoning: Wrong retrieved context produces wrong answers. If your retrieval system fetches incorrect documents, the model confidently outputs wrong information.

2. Retrieval quality: You must find the right documents for every query, forever.

3. No permanent fix: You're overriding the model, not correcting it. The underlying error remains.

4. Discovery problem: How do you know when to inject a correction if you don't already know the answer is wrong?

Lesson: RAG trades permanent weight corruption for runtime infrastructure complexity and new attack surfaces.

────────────────────────────────────────────────────────────────────────

PART IV: THE INSIGHT

────────────────────────────────────────────────────────────────────────

WHAT WE ACTUALLY LEARNED

Our failures revealed something important:

  KNOWLEDGE IN TRANSFORMERS IS NOT ADDRESSABLE. IT IS ONLY ACTIVATABLE.

You cannot edit a fact without collateral damage.
You cannot move a concept without the model compensating.
You can sometimes nudge execution toward an already-learned circuit.

This explains everything:

Why embedding modification failed:
  Embeddings aren't addresses. They're initial conditions in a dynamical
  system. By layer 8-12, the original embedding has been projected, mixed,
  normalized, and entangled. The "meaning" lives in circuits, not vectors.

Why ROME caused collateral damage:
  Weights aren't fact storage. They're shared computation paths. Editing
  one path edits all traffic through that path.

Why RAG works despite being ugly:
  RAG doesn't change knowledge. It biases execution. It activates circuits
  that already exist.

Why centroid injection recovers correct answers:
  Injection doesn't add knowledge. It activates knowledge. The centroid
  pushes the model toward circuits that already encode the right answer.

────────────────────────────────────────────────────────────────────────

THE GEOMETRY OF MEANING: RATIOS, NOT MAGNITUDES

A deeper insight emerged from our experiments: semantic identity is encoded in the ratios between dimensions, not in magnitudes.

Consider what layer normalization does: it preserves the relative relationships between dimensions while normalizing the overall magnitude. If meaning were encoded in magnitudes, layer normalization would destroy it. Instead, layer normalization is essential to transformer function.

This explains why cosine similarity is the right metric for semantic comparison. Cosine similarity measures angle—the ratio pattern between dimensions—and ignores magnitude entirely. Two vectors with the same direction but different lengths have cosine similarity of 1.0. They represent the same concept.

Our semantic sink finding (0.9995 cosine similarity between dantrolene and cooling) now has a geometric interpretation: these concepts have nearly identical ratio patterns across dimensions. They point in the same direction. No amount of scaling can separate them, because scaling doesn't change direction.

This also explains why our 1400% embedding modification had no effect. We changed the magnitude dramatically, but layer normalization preserved the ratios. The direction—and therefore the meaning—remained unchanged. We were trying to solve a directional problem with magnitude changes.

But magnitude does matter for one thing: influence.

When we inject an engram into the hidden state:

  hidden_new = hidden_original + scale * engram

We are blending two directions. Before layer normalization:
- Small scale: hidden_original dominates, output direction approximately equals original
- Large scale: engram dominates, output direction approximately equals engram

Layer normalization then normalizes the magnitude, but the blended direction depends on the scale. This is why our engram injection experiments showed effects at different strengths—we were changing the blend ratio between the original hidden state direction and the engram direction.

Think of it like mixing paint. Red is red, blue is blue—their identities are fixed. But the ratio of red to blue determines the output color. Scaling the engram changes how much it pulls the combined representation toward its direction.

This distinction clarifies the design space:
- To change what a concept means: change its direction (ratios between dimensions)
- To change how strongly a concept influences output: change its magnitude relative to other signals

Post-hoc embedding modification fails because layer normalization preserves direction. You cannot rotate a vector by stretching it. But injection before layer normalization succeeds (sometimes) because it introduces a competing direction that shifts the blend.

The semantic sink problem is fundamentally directional. Dantrolene and cooling point the same way. Fixing this requires rotating one of them—changing the ratio pattern—not scaling. And in a trained network, the downstream layers have learned to interpret directions as they are. Rotation would require retraining.

────────────────────────────────────────────────────────────────────────

THE CONSERVATION OF INFORMATION FLOW

There is another way to understand why knowledge is distributed and why editing fails: think of information as water flowing through pipes.

In a fully connected neural network, each layer computes weighted sums. The output of a layer is the sum of all its inputs multiplied by learned weights. This is addition—a linear combination.

Consider training two networks on the same task:
- Network A: 9 nodes in the hidden layer
- Network B: 7 nodes in the hidden layer

Both networks, trained to convergence, will produce the same output distribution. The softmax at the end will give the same probabilities. Why?

Because the task determines how much "water" needs to flow. The pipes (nodes) just determine how that flow is distributed. More pipes or fewer pipes—the same total water gets through. Each pipe carries a different fraction, but the sum is constant.

With 9 nodes, each carries a smaller portion of the signal. With 7 nodes, each carries more. The weights adjust to compensate. The network learns whatever distribution produces the correct sums at the output.

This is why you cannot identify "the dantrolene node" or "the cooling node." The concept is not stored in any single pipe. It is the total flow—distributed across all pipes, combined at the output. Different architectures produce different distributions of the same flow.

And this is why editing individual weights causes collateral damage. You are trying to change the total flow by messing with one pipe. But the network learned to achieve that flow through the combination of ALL pipes. Changing one changes the sum. Other concepts that also flow through that pipe are affected.

The task determines the flow. The architecture determines the plumbing. The weights learn the distribution. The output is invariant to the specific decomposition—only the total matters.

This principle—conservation of information flow—explains:

Why knowledge is distributed:
  There is no "storage location." There is only flow through weighted sums.
  Any configuration of pipes that produces the correct sums is valid.

Why editing is hard:
  You cannot change the flow to one destination without affecting flow to others.
  The pipes are shared. The sums are entangled.

Why different architectures learn the same function:
  The function is the input-output mapping. The internals are arbitrary.
  Nine pipes or seven pipes—same water delivered.

Why the model "knows" things it cannot reliably access:
  The knowledge exists as potential flow. But routing determines which sums
  get computed for a given input. The water is there; the valves may be wrong.

The fully connected architecture that makes neural networks powerful—the ability to learn any weighted combination—is also what makes them resistant to surgical modification. Everything connects to everything. Every edit propagates through the sums.

────────────────────────────────────────────────────────────────────────

WHAT ACTUALLY MIGHT WORK

Our failures point toward what might succeed:

ACTIVATION PATHWAY STEERING

Instead of editing what the model knows, influence how it accesses what it knows. Late-layer activation interventions that bias routing toward correct circuits without modifying weights.

This is what our earlier centroid/engram work demonstrated:
  - Geometry as analysis is valuable (identifies semantic sinks)
  - Geometry as permanent intervention fails
  - Geometry as runtime steering may work

The viable intervention class operates on routing and organization, not factual content.

────────────────────────────────────────────────────────────────────────

PERTURBATION TESTING: DETECTING SINKS WITHOUT FIXING THEM

If we cannot reliably fix semantic sinks, can we at least detect them?

The answer is yes, with an important caveat.

When we apply an activation perturbation (engram injection, steering vector, or centroid shift) and the model's output changes significantly, this tells us something important: the model was sitting on a decision boundary. A small push in activation space flipped the answer.

This is the signature of a semantic sink. The model had two (or more) competing outputs with nearly equal probability, and the perturbation tipped the balance.

THE DETECTION METHOD

  1. Run the query normally, record the output
  2. Apply a domain-relevant perturbation
  3. Run the query again, record the new output
  4. Compare: Did the answer flip?

  If the answer flips:
    The output is UNSTABLE
    Flag for human review
    Do not trust the original answer

  If the answer holds:
    The output is STABLE
    The model is not in a detectable sink
    (This does not guarantee correctness)

WHAT THIS DOES AND DOES NOT TELL US

Detection works because semantic sinks are regions where small perturbations cause large output changes. If you're in a sink, poking the model will reveal it.

But stability is not correctness. A model can be confidently wrong and stable. The perturbation test catches one failure mode (decision boundary uncertainty) but not others (confident hallucination, knowledge gaps, systematic bias).

To be explicit about scope: Perturbation testing detects errors caused by routing instability, not errors caused by missing knowledge, systematic bias, or confidently wrong priors. It is one tool, not a complete solution.

Think of it as a necessary but not sufficient condition:
  - Unstable output → definitely flag it
  - Stable output → still might be wrong, but not from this cause

WHAT COUNTS AS A DOMAIN-RELEVANT PERTURBATION

Not all perturbations are equal. A domain perturbation is a small, medically grounded activation shift derived from in-distribution clinical representations, not an arbitrary noise vector.

Effective perturbations we tested:
  - Engram injection: Mean-pooled hidden states from relevant medical text
  - Steering vectors: Directional vectors between related clinical concepts
  - Centroid shifts: Difference between specific and general treatment contexts

What matters is that the perturbation activates circuits in the same domain as the query. Random noise would not reveal semantic sinks—it would just add noise. The perturbation must be semantically meaningful to probe the relevant decision boundary.

OPERATIONAL SAFETY CONSTRAINTS

Perturbation strength must be fixed and validated offline. Adaptive or query-dependent perturbations introduce adversarial risk—an attacker could craft inputs that exploit the perturbation mechanism itself.

The validated approach:
  1. Calibrate perturbation strength on a held-out validation set
  2. Fix the strength for deployment (we found 0.01 optimal)
  3. Never adjust strength based on query content
  4. Monitor for distribution shift that might invalidate calibration

This constraint reduces flexibility but ensures the detection mechanism cannot be gamed.

WHY WE CANNOT USE PERTURBATION TO FIX

A natural question: if perturbation can flip a wrong answer to correct, why not use it as a fix?

Because perturbation is unpredictable. In our experiments:
  - Sometimes perturbation flipped wrong → correct
  - Sometimes perturbation flipped correct → wrong
  - We found no reliable way to predict which would happen

Applying a perturbation without knowing its effect is gambling with patient safety. But using perturbation to detect uncertainty—and then routing to human review—is a valid safety measure.

The perturbation becomes a probe, not a fix.

EMPIRICAL RESULTS: FINDING THE DETECTION SWEET SPOT

We benchmarked perturbation detection across 12 medical questions with known correct answers, testing 10 different perturbation strengths. The goal: find a strength where we catch wrong answers (sensitivity) without flagging too many correct answers (false positive rate).

Baseline model performance: 5 correct, 5 wrong, 2 unclear.

Results by perturbation strength:

- Strength 0.001: Sensitivity 0%, False Positive 20%, Precision 0%
- Strength 0.005: Sensitivity 0%, False Positive 20%, Precision 0%
- Strength 0.010: Sensitivity 40%, False Positive 20%, Precision 67% — SWEET SPOT
- Strength 0.015: Sensitivity 40%, False Positive 40%, Precision 50%
- Strength 0.020: Sensitivity 40%, False Positive 60%, Precision 40%
- Strength 0.030: Sensitivity 60%, False Positive 80%, Precision 38%
- Strength 0.050: Sensitivity 60%, False Positive 100%, Precision 33%
- Strength 0.100: Sensitivity 80%, False Positive 100%, Precision 36%

The sweet spot is at strength 0.01:
  - Sensitivity: 40% (catches 2 of 5 wrong answers)
  - False positive rate: 20% (flags 1 of 5 correct answers)
  - Precision: 67% (when we flag, 2/3 are actually wrong)

This is a weak but real signal. The flag carries more information than random chance.

Key observations:

1. Too weak (< 0.01): No detection, but still some false positives from minor output variations.

2. Sweet spot (0.01): Sensitivity exceeds false positive rate. Net positive signal.

3. Too strong (> 0.02): False positives rise faster than sensitivity. Signal degrades.

4. Very strong (> 0.05): All outputs flag, rendering the test useless.

THE SIGNAL IS WEAK BUT WORTH INCLUDING

At the optimal strength:
  - We catch 40% of wrong answers we would otherwise miss
  - We false-flag 20% of correct answers (acceptable review burden)
  - When flagged, there's a 67% chance it's actually wrong

This is not a solution. We still miss 60% of wrong answers. But in high-stakes domains where the cost of errors is severe, even a weak signal that catches some errors is valuable.

The asymmetric cost argument:
  - Cost of false positive: Extra human review (time, money)
  - Cost of false negative: Wrong medical advice (potential patient harm)

When missing errors has catastrophic consequences, a 40% catch rate justifies the 20% false positive burden.

Recommendation: Include perturbation testing as one signal in a multi-factor safety pipeline, not as a standalone solution. Combine with confidence thresholds, retrieval verification, and other checks. Never interpret "not flagged" as "safe."

PRACTICAL APPLICATION

For a medical AI deployment:

1. Query arrives
2. Generate response (output A)
3. Apply domain perturbation
4. Generate again (output B)
5. Compare A and B
   - If DIFFERENT: FLAG for human review
   - If SAME: Continue (apply other safety checks)

This adds latency (two inference passes) but catches a real failure mode that standard confidence scores miss. The model may report high confidence while sitting on a semantic sink—the perturbation test reveals the instability that confidence scores hide.

────────────────────────────────────────────────────────────────────────

IMPLICATIONS FOR MEDICAL AI

1. Model editing is not ready for production medical use.
   Current techniques cannot reliably fix one fact without breaking others.

2. RAG is safer but not a solution.
   It's a workaround that shifts the problem to retrieval quality.

3. The fundamental issue is architectural.
   Transformer knowledge is distributed and entangled. This may be an
   inherent property of current architectures trained via gradient descent,
   not a bug to fix.

4. Validation cannot be automated away.
   For high-stakes domains, human expert review remains essential.

5. Post-deployment corrections must target activation, not knowledge.
   If the model knows the right answer, help it access that answer.
   Don't try to change what it knows.

────────────────────────────────────────────────────────────────────────

THE OPTIMIZATION TARGET PROBLEM

The use case determines what you should optimize for. We are optimizing for the wrong thing.

Current AI development optimizes for average performance on benchmarks. A model that scores 95% accuracy is considered excellent. But where does the 5% failure come from?

It comes from the edge cases. The rare presentations. The unusual combinations. The cases with weak training signal and small weights.

For movie recommendations, this is fine:
- Wrong answer = mild annoyance
- 5% error rate? Show another movie. Nobody cares.

For medical diagnosis, this is catastrophic:
- Wrong answer = potential death
- 5% error rate on edge cases = the hardest cases fail
- The patients who most need AI help are the ones it fails

Different domains require different loss functions:

- Entertainment: minimize average error
- Medical: minimize maximum harm
- Financial: minimize tail risk
- Legal: minimize false positives

The problem: we build general-purpose models optimized for benchmarks, then deploy them in high-stakes domains. GPT was not optimized for medical precision. It was optimized for next-token prediction on internet text. Then we ask it medical questions and hope for the best.

Model compression makes this worse. Pruning research celebrates "95% of weights removed with only 1% accuracy drop." But that 1% accuracy drop concentrates in the edge cases—exactly where the stakes are highest.

"1% accuracy drop" is a statistic. "You're in the 1%" is a death sentence.

And it is not a random 1%. It is the rare cases, the atypical presentations, the patients with malignant hyperthermia instead of heat stroke. The cases that needed the small weights—the ones pruning removes first.

The right approach for medical AI:
- Domain-specific optimization from the start
- Loss functions that penalize edge-case failures heavily
- Evaluation on rare and critical cases, not just benchmarks
- Preserve the small weights that handle unusual presentations
- Accept higher compute costs in exchange for edge-case reliability

This is not "fine-tune GPT and deploy." It is "build medical AI as medical AI, with medical objectives, evaluated on medical criteria."

The stakes determine the optimization target. When the cost of failure is death, optimize for precision on the cases that kill—not average performance on the cases doctors already get right.

────────────────────────────────────────────────────────────────────────

THE TWO TYPES OF SMALL WEIGHTS

Not all small weights are equal. A weight with value 0.001 could be one of two things:

1. Optimal: The network learned this connection should contribute a tiny but important amount. This weight is at equilibrium. It encodes edge-case information.

2. Transient: This weight is dying. Given more training time, it would reach zero. Training stopped before it finished converging. This is garbage.

You cannot tell the difference by looking at the value. Both are 0.001. One is precious signal. One is incomplete pruning by gradient descent.

This explains the pruning paradox. Sometimes aggressive pruning barely hurts accuracy—you removed the transient garbage, completing the work training started. Sometimes pruning devastates edge cases—you removed optimal small weights that encoded critical distinctions.

Magnitude-based pruning treats both types the same. It is a crude heuristic that works on average but makes unpredictable errors.

The principled approach would be trajectory-based pruning:
- Track weight values during training
- Weight declining toward zero → transient, safe to prune
- Weight stabilized at small value → optimal, preserve it

Remove garbage, accuracy should stay the same or improve. Remove information, accuracy should worsen—especially on edge cases.

This is testable but impractical. Tracking trajectories for billions of weights requires massive storage and computation for probably marginal improvement over simple magnitude pruning.

An approximation: examine gradient magnitude at the final checkpoint. Near-zero gradient suggests equilibrium (keep the weight). Non-zero gradient still pushing toward zero suggests the weight is still dying (safe to prune). But even this adds cost.

For most applications, the pragmatic approach wins: prune by magnitude, accept some edge-case loss, move on. The cost of principled pruning exceeds the benefit.

But for medical AI, edge cases are where patients die. The cost-benefit calculation is different. If preserving small stable weights saves lives, the computational cost of trajectory analysis may be justified.

Different domains, different tradeoffs. Entertainment AI can use crude heuristics. Medical AI may need to pay the cost of precision.

This connects to a broader theme: fully connected networks are bizarre topologies where everything connects to everything. The structure is entirely in the weights. Some connections end up large (important paths), some end up near zero (unused paths), and some end up small but stable (edge-case paths). Understanding which is which requires understanding the training dynamics, not just the final snapshot.

Recent work on neuron merging offers an alternative to pruning. Instead of zeroing out small weights and losing their contribution, merge them—combine small weights into fewer, larger weights that preserve the total signal. This maintains the flow while reducing the connection count. Research shows merged models preserve accuracy better than pruned models at the same compression ratio, precisely because merging keeps the information that pruning discards.

────────────────────────────────────────────────────────────────────────

WHAT WOULD ACTUALLY HELP

1. Training-time interventions
   Catch and correct errors during training, not after deployment.

2. Explicit knowledge separation
   Architectures that store facts in retrievable, editable databases
   rather than distributed weights.

3. Uncertainty calibration
   Models that know what they don't know, enabling targeted human review.

4. Continuous validation pipelines
   Automated comparison against authoritative sources, flagging drift.

5. Activation steering research
   Develop reliable methods for runtime routing intervention that don't
   require weight modification.

6. Perturbation-based uncertainty detection
   Use activation perturbation not as a fix but as a probe. Flag outputs
   that flip under perturbation for human review. This catches semantic
   sink failures that confidence scores miss.

7. Protocol-based architectures
   Sidestep the embedding problem entirely. Use categorical routing to
   explicit protocols rather than similarity search. The LLM reasons;
   human-written protocols hold the knowledge. Self-correcting through
   text file updates, not weight modification.

8. N-gram embeddings for compound concepts
   Part of the semantic sink problem may be a tokenization problem.
   "Malignant hyperthermia" and "heat stroke" are distinct conditions
   requiring different treatments, but they share proximity to
   "hyperthermia" in embedding space because the vocabulary doesn't
   distinguish them as compound concepts.

   Recent work (DeepSeek's Engram, January 2026) demonstrates N-gram
   embedding tables that give token pairs their own learned vectors.
   The bigram ("malignant", "hyperthermia") would get a distinct
   embedding pointing toward dantrolene, separate from general
   hyperthermia contexts pointing toward cooling.

   Medical terminology is dense with compound concepts: diagnoses,
   drug-condition pairs, anatomical structures, treatment protocols.
   A domain-specific N-gram table could provide disambiguation at the
   representation level—giving distinct concepts distinct directions
   rather than forcing the transformer to disambiguate through attention
   every time. This won't solve all routing failures, but it addresses
   cases where the sink exists because compound concepts lack distinct
   representations.

None of these are simple. All require fundamental changes to how medical AI systems are built and deployed.

────────────────────────────────────────────────────────────────────────

ALTERNATIVE: PROTOCOL-BASED ARCHITECTURES

There is another path that sidesteps the semantic sink problem entirely.

The core issue with vector databases and embedding-based retrieval is that similarity does not equal correctness. A query about malignant hyperthermia will retrieve things geometrically close to "malignant hyperthermia" in embedding space—but geometric proximity reflects statistical co-occurrence in training data, not clinical accuracy.

You get neighbors, not answers.

Asking "what treats malignant hyperthermia?" might retrieve:
  - "Malignant hyperthermia is a serious condition" (high similarity, not an answer)
  - "Hyperthermia treatment involves cooling" (high similarity, wrong answer)
  - "Dantrolene treats malignant hyperthermia" (lower similarity, correct answer)

The embedding space clusters by topic, not by correctness. This is the same geometric pathology we identified as semantic sinks, now appearing in the retrieval layer.

THE PROTOCOL ALTERNATIVE

Instead of similarity search, use categorical routing with explicit protocols.

A protocol is a human-readable text file containing explicit procedures for a specific situation. The system maintains a library of protocols, indexed by type.

The architecture:

  1. Prompt arrives
  2. LLM routes: "Which protocols apply?" (categorical, not similarity-based)
  3. Selected protocols loaded into context
  4. LLM executes with explicit guidance
  5. If error occurs, error protocol activates
  6. LLM analyzes failure and proposes protocol update
  7. Protocol is updated (text file edit, human-reviewable)
  8. System improves over time

The key differences:

Similarity search:
  - "Find something close"
  - Returns neighbors
  - Opaque embeddings
  - Static (no learning)
  - Hope it's relevant

Protocol routing:
  - "Which type is this?"
  - Returns procedures
  - Human-readable text
  - Self-correcting
  - Explicitly prescriptive

WHY ROUTING IS EASIER THAN ANSWERING

The question "What treats malignant hyperthermia?" is hard.

The question "Is this a malignant hyperthermia case?" is easier.

Once the system correctly identifies the case type, the protocol provides the answer explicitly: "Administer dantrolene 2.5 mg/kg IV." No similarity search, no geometric ambiguity, no semantic sink.

The LLM does the reasoning—understanding context, handling edge cases, recognizing patterns. The protocols hold the knowledge—verified, explicit, auditable.

This separation is crucial. We established that knowledge in transformers is not reliably addressable. But reasoning is what transformers do well. Use each component for what it's good at.

SELF-CORRECTION WITHOUT WEIGHT MODIFICATION

When errors occur, the protocol system can learn without touching model weights.

1. Error occurs
2. Error protocol activates
3. LLM analyzes: "What went wrong?"
4. LLM proposes: "Update protocol X, section Y"
5. Protocol updated (text file diff)
6. Human reviews change
7. System now handles this case correctly

Compare this to weight editing:

Weight editing:
  - Collateral damage
  - Opaque
  - Hard to undo
  - Affects all inputs
  - 80% unrelated fact corruption

Protocol editing:
  - Isolated change
  - Human-readable diff
  - Version controlled
  - Affects one protocol
  - 0% unrelated fact corruption

The protocols accumulate institutional knowledge. Each error makes the system better. The changes are auditable—a human can review exactly what was modified before accepting the update.

This is what "learning" should look like in high-stakes systems: explicit, inspectable, reversible changes to human-readable procedures.

SCALING THE PROTOCOL LIBRARY

As the protocol library grows:

  Small library:    Full index in context, LLM selects directly
  Medium library:   LLM outputs protocol numbers, then load selected protocols
  Large library:    Pre-filter with keywords/rules, LLM routes from candidates

At scale, you combine:
  - Keyword matching: fast, exact, cheap
  - Rule-based filtering: categorical, verifiable
  - LLM routing: understanding intent, handling edge cases
  - Protocols: explicit, correct, auditable

No embeddings. No hoping the geometry is right. Each tool used where it's strong.

WHY THIS ISN'T FASHIONABLE

This approach works. It's also boring.

There are no embeddings to visualize. No neural architecture to diagram. No benchmark to beat. Just text files, routing logic, and an error correction loop.

The field rewards complexity. Papers about novel embedding spaces and learned similarity metrics get published. Papers about "we wrote down procedures and the system follows them" do not.

But in high-stakes domains, boring and correct beats clever and wrong.

The protocols are the knowledge—human-written, human-verified, human-readable. The LLM is the reasoning engine—flexible, contextual, capable. Separated, each does what it's good at. Combined, they form a system that can actually be trusted.

────────────────────────────────────────────────────────────────────────

PART V: REFLECTION

────────────────────────────────────────────────────────────────────────

A NOTE ON OPEN NOTEBOOK RESEARCH

This work was conducted as open notebook research. We published findings as we made them, including hypotheses that later proved wrong.

We published the geometric correction proposal on Medium before testing it. When embedding modification failed, we took the article down. This report exists because we documented our failures instead of quietly abandoning them.

Benefits:
  - Failures get recorded (most negative results never published)
  - Reasoning is visible (lessons become transferable)
  - Honest science (research is messy; polish misrepresents it)

Problems:
  - We published wrong ideas before knowing they were wrong
  - Reputational risk in admitting error
  - Incomplete thinking becomes permanent

We think the tradeoff is worth it. The cost of others repeating our mistakes exceeds the cost of our embarrassment.

If you read our earlier work and tried to implement geometric correction before reading this report, we may have wasted your time. We're sorry. This is the price of working in public.

────────────────────────────────────────────────────────────────────────

CONCLUSION

We began hoping to find a surgical fix for medical AI errors.

We found instead that the patient's organs are all connected in ways that make surgery dangerous.

The semantic sink diagnosis stands: distinct medical concepts really do collapse into indistinguishable regions of representation space, and this really does cause routing failures.

The proposed correction failed: you cannot fix routing by moving embeddings, and you cannot edit knowledge without collateral damage.

The insight that emerged: knowledge in transformers is not addressable, only activatable. Post-deployment interventions must operate on routing and organization, not on factual content itself.

Two paths forward:

First, perturbation testing can detect (though not fix) semantic sink errors. A weak signal, but real—40% of wrong answers caught at the cost of 20% false positives. Worth including in high-stakes pipelines where the cost of errors is severe.

Second, protocol-based architectures sidestep the problem entirely. Rather than hoping embeddings capture correctness, use explicit protocols that humans write, verify, and update. The LLM reasons; the protocols know. This separation uses each component for what it's good at and produces a system that can actually be trusted.

The problem is harder than it looks. We hope this complete account—hypothesis, experiment, failure, and insight—helps others avoid our false starts and build on what actually works.

────────────────────────────────────────────────────────────────────────

Research conducted January 2026.
Code and samples available at: github.com/MikeyBeez/engrams

────────────────────────────────────────────────────────────────────────

APPENDIX: EXPERIMENTAL DETAILS

Models tested:
  - Qwen2.5-3B (primary experiments)
  - Qwen2.5-7B (memory-limited testing)

Key measurements:
  - Semantic sink similarity: 0.9995 (dantrolene/cooling centroids)
  - Embedding modification: 1400% change, 0.999 centroid similarity preserved
  - ROME locality: 80% unrelated fact corruption (multi-layer)
  - RAG locality: 0% content corruption
  - Perturbation detection (optimal): 40% sensitivity, 20% false positive rate, 67% precision

Code:
  - Centroid extraction: engrams/extractor.py
  - Embedding experiments: scripts/test_geometric_correction.py
  - ROME implementation: src/rome_edit.py
  - Locality testing: scripts/rome_locality_test.py
  - RAG comparison: scripts/rag_vs_rome_test.py
  - Steering gamma sweep: scripts/steering_gamma_sweep.py
  - Perturbation help/hurt test: scripts/steering_help_hurt_test.py
  - Perturbation detection benchmark: scripts/perturbation_detection_benchmark.py
  - Perturbation strength sweep: scripts/perturbation_strength_sweep.py
